# -*- coding: utf-8 -*-
"""SimpleEmbedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IJNNJ1JM57iLgj1oawGw5nxqqn7ndCDm

## 1 SIMPLE EMBEDDING MODEL

This code creates a word embedding from two sample sentences by training samples based on **next word prediction task**. Embeddings are vectors that represent the closeness of words in higher dimensions. Here we attempt to generate simple embeddings for the two sentences "statistics is great", "mathematics is great" using an embedding dimension of size 2, meaning we are generating embedding vector of size 2.

# 1.1 IMPORT REQUIRED LIBRARIES
"""

import numpy as np
import keras
from keras import layers, Sequential
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
from tensorflow.keras.utils import pad_sequences
from keras.layers import Embedding, Flatten, Dense

"""# 1.2 TOKENIZE THE SENTENCES AND GENERATE WORD INDEX MAPPING"""

# Sentences to encode
sentences = ["statistics is great", "mathematics is great"]

# Create the tokenizer
tokenizer = Tokenizer()

# Fit the tokenizer on the sentences
tokenizer.fit_on_texts(sentences)

# Convert sentences to sequences of integers
sequences = tokenizer.texts_to_sequences(sentences)

# Print the sequences
print("Tokenized sentences (as sequences of integers):")
print(sequences)

# To see the word index mapping
word_index = tokenizer.word_index
print("\nWord index (mapping of words to integers):")
print(word_index)

"""## 2 GENERATE TRAINING DATA

We need to generate training data for next word prediction. We do this by iterating through the sequence list and creating two new lists X and Y which form basis for training.
"""

# Total vocabulary size
vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of zero indexing

# Prepare the dataset for training a next word prediction model
X = []
Y = []

for sequence in sequences:
    for i in range(1, len(sequence)):
        # Take the first i elements as the input and the i-th element as the output
        x_sequence, y_sequence = sequence[:i], sequence[i]
        X.append(x_sequence)
        Y.append(y_sequence)
print("X is", X)
print("Y is", Y)
# Pad the input sequences so that they all have the same length
max_length = max([len(seq) for seq in X])
X = pad_sequences(X, maxlen=max_length, padding='pre')

# One-hot encode the output sequences
Y = to_categorical(Y, num_classes=vocab_size)

# Now X and Y are ready to be used as input and output for training a neural network
print("Input (X):")
print(X)
print("\nOutput (Y):")
print(Y)

"""# 3 BUILD A NEURAL NETWORK TO TRAIN THE MODEL

We train a simple neural network which includes a) An embedding layer (whose weights represent embedding vectors), b) A flatten layer that flattens the input from embedding layer, and c) Output layer which applies a "Softmax" to predict the next word.
"""

# Input data (X) and labels (Y) from the previous output
X = np.array(X)
Y = np.array(Y)

# Define the model
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=2, input_length=max_length))
model.add(Flatten()) # Flatten the embeddings to connect to a dense layer
model.add(Dense(units=vocab_size, activation='softmax')) # Output layer with softmax activation

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X, Y, epochs=1000, verbose=0)  # Set verbose to 1 if you want to see the training progress

"""# 4 RETRIEVE EMBEDDINGS

We retrieve the embeddings from the trained model. This is done by accessing the model weights stored in layer 0 which is the embedding layer.
"""

# Retrieve the embeddings (weights of the first layer)
embeddings = model.layers[0].get_weights()[0]

# Print the embedding values for each word
print("Embedding values for each word:")
for word, i in word_index.items():
    print(f"{word}: {embeddings[i]}")

"""# 5 PLOT THE EMBEDDING VECTOR

This is the most exciting part. Here, we plot the word embeddings and the corresponding words in a scatterplot. Notice the closeness of words "Mathematics" and "Statistics"
"""

import matplotlib.pyplot as plt
# Plotting the embeddings
plt.figure(figsize=(8, 6))
for word, i in word_index.items():
    plt.scatter(embeddings[i, 0], embeddings[i, 1], label=word)

# Annotate each point in 2D space with the word it corresponds to
for word, i in word_index.items():
    plt.text(embeddings[i, 0], embeddings[i, 1], word)

plt.title('Word Embeddings')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()
